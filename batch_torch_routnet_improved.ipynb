{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g2F4VUMlkju",
        "outputId": "3122fafb-cd6c-423d-bf10-a8eb482eb875"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/MyDrive/Fall23/Capstone Research/new_efforts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMbQkotnWfTf",
        "outputId": "0d4d0249-efd0-4eb1-c162-ab23e7e2ed6c"
      },
      "outputs": [],
      "source": [
        "# !pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0CUkGreSOjY",
        "outputId": "982b97b2-6cce-414d-a820-a13230fd1db8"
      },
      "outputs": [],
      "source": [
        "# !python3 \"/content/drive/MyDrive/Fall23/Capstone Research/new_efforts/ITU-ML5G-PS-007-GNN-m0b1us/std_train.py\" -ds CBR+MB --ckpt-path moblus_mb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WHCFjMAb7Uep"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from torchmetrics.regression import MeanAbsolutePercentageError\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as  np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "# from torchrec import JaggedTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FHYHsMy0F-tl"
      },
      "outputs": [],
      "source": [
        "ds_path = \"extracted_moblus_devices_cv/3\"\n",
        "ds_val = \"extracted_moblus_devices_cv/3\"\n",
        "ds_train = tf.data.Dataset.load(f\"{ds_path}/training\", compression=\"GZIP\")\n",
        "ds_val = tf.data.Dataset.load(f\"{ds_path}/validation\", compression=\"GZIP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "MSJSGWMMSHj4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert TF dataset to PyTorch dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tf_dataset):\n",
        "        self.tf_dataset = tf_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        # Calculate the length of the dataset\n",
        "        # Implement according to the structure of your dataset\n",
        "        return len(self.tf_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve an item from the dataset\n",
        "        # Implement according to the structure of your dataset\n",
        "        return self.tf_dataset[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "57d9BPpH3H8Q"
      },
      "outputs": [],
      "source": [
        "train_list = []\n",
        "# X_graphs = []\n",
        "for item in ds_train:\n",
        "    # Assuming the elements in the dataset are tensors, convert them into a format compatible with PyTorch\n",
        "    train_list.append((item[0], item[1].numpy()))\n",
        "\n",
        "val_list = []\n",
        "# X_graphs = []\n",
        "for item in ds_val:\n",
        "    # Assuming the elements in the dataset are tensors, convert them into a format compatible with PyTorch\n",
        "    val_list.append((item[0], item[1].numpy()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "XwiMhCnkDi2h"
      },
      "outputs": [],
      "source": [
        "mean_std_scores_fields = {\n",
        "        \"flow_traffic\",\n",
        "        \"flow_packets\",\n",
        "        \"flow_pkts_per_burst\",\n",
        "        \"flow_bitrate_per_burst\",\n",
        "        \"flow_packet_size\",\n",
        "        \"flow_p90PktSize\",\n",
        "        \"rate\",\n",
        "        \"flow_ipg_mean\",\n",
        "        \"ibg\",\n",
        "        \"flow_ipg_var\",\n",
        "        \"link_capacity\",\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kB6cke0BfPZ",
        "outputId": "77b237c7-3e74-4752-e799-b16397457038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'flow_bitrate_per_burst',\n",
              " 'flow_ipg_mean',\n",
              " 'flow_ipg_var',\n",
              " 'flow_p90PktSize',\n",
              " 'flow_packet_size',\n",
              " 'flow_packets',\n",
              " 'flow_pkts_per_burst',\n",
              " 'flow_traffic',\n",
              " 'ibg',\n",
              " 'link_capacity',\n",
              " 'rate'}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_std_scores_fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "kuJTj7zVAvnM"
      },
      "outputs": [],
      "source": [
        "def get_mean_std_dict(\n",
        "    ds: tf.data.Dataset, params, include_y = None\n",
        "):\n",
        "    \"\"\"Get the min and the max-min for different parameters of a dataset. Later used by the models for the min-max normalization.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ds : tf.data.Dataset\n",
        "        Training dataset where to base the min-max normalization from.\n",
        "\n",
        "    params : List[str]\n",
        "        List of strings indicating the parameters to extract the features from.\n",
        "\n",
        "    include_y : Optional[str], optional\n",
        "        Indicates if to also extract the features of the output variable.\n",
        "        Inputs indicate the string key used on the return dict. If None, it is not included.\n",
        "        By default None.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict[str, Tuple[np.ndarray, np.ndarray]]\n",
        "        Dictionary containing the values needed for the min-max normalization.\n",
        "        The first value is the min value of the parameter, and the second is 1 / (max - min).\n",
        "    \"\"\"\n",
        "\n",
        "    # Use first sample to get the shape of the tensors\n",
        "    iter_ds = iter(ds)\n",
        "    sample, label = next(iter_ds)\n",
        "    params_lists = {param: sample[param].numpy() for param in params}\n",
        "    if include_y:\n",
        "        params_lists[include_y] = label.numpy()\n",
        "\n",
        "    # Include the rest of the samples\n",
        "    for sample, label in iter_ds:\n",
        "        for param in params:\n",
        "            params_lists[param] = np.concatenate(\n",
        "                (params_lists[param], sample[param].numpy()), axis=0\n",
        "            )\n",
        "        if include_y:\n",
        "            params_lists[include_y] = np.concatenate(\n",
        "                (params_lists[include_y], label.numpy()), axis=0\n",
        "            )\n",
        "\n",
        "    scores = dict()\n",
        "    for param, param_list in params_lists.items():\n",
        "        mean_val = np.mean(param_list, axis=0)\n",
        "        std_val = np.std(param_list, axis=0)\n",
        "\n",
        "        if all(std_val) == 0:\n",
        "            scores[param] = [mean_val, std_val]\n",
        "        else:\n",
        "            scores[param] = [mean_val, 1/std_val]\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 2\n",
            "2 4\n",
            "4 6\n",
            "6 8\n",
            "8 10\n",
            "10 12\n",
            "12 14\n",
            "14 16\n",
            "16 18\n",
            "18 20\n",
            "20 22\n",
            "22 24\n",
            "24 26\n",
            "26 28\n",
            "28 30\n",
            "30 32\n",
            "32 34\n",
            "34 36\n",
            "36 38\n",
            "38 40\n",
            "40 42\n",
            "42 44\n",
            "44 46\n",
            "46 48\n",
            "48 50\n",
            "50 52\n",
            "52 54\n",
            "54 56\n",
            "56 58\n",
            "58 60\n",
            "60 62\n",
            "62 64\n",
            "64 66\n",
            "66 68\n",
            "68 70\n",
            "70 72\n",
            "72 74\n",
            "74 76\n",
            "76 78\n",
            "78 80\n",
            "80 82\n",
            "82 84\n",
            "84 86\n",
            "86 88\n",
            "88 90\n",
            "90 92\n",
            "92 94\n",
            "94 96\n",
            "96 98\n",
            "98 100\n",
            "100 102\n",
            "102 104\n",
            "104 106\n",
            "106 108\n",
            "108 110\n",
            "110 112\n",
            "112 114\n",
            "114 116\n",
            "116 118\n",
            "118 120\n",
            "120 122\n",
            "122 124\n",
            "124 126\n",
            "126 128\n",
            "128 130\n",
            "130 132\n",
            "132 134\n",
            "134 136\n",
            "136 138\n",
            "138 140\n",
            "140 142\n",
            "142 144\n",
            "144 146\n",
            "146 148\n",
            "148 150\n",
            "150 152\n",
            "152 154\n",
            "154 156\n",
            "156 158\n",
            "158 160\n",
            "160 162\n",
            "162 164\n",
            "164 166\n",
            "166 168\n",
            "168 170\n",
            "170 172\n",
            "172 174\n",
            "174 176\n",
            "176 178\n",
            "178 180\n",
            "180 182\n",
            "182 184\n",
            "184 186\n",
            "186 188\n",
            "188 190\n",
            "190 192\n",
            "192 194\n",
            "194 196\n",
            "196 198\n",
            "198 200\n",
            "200 202\n",
            "202 204\n",
            "204 206\n",
            "206 208\n",
            "208 210\n",
            "210 212\n",
            "212 214\n",
            "214 216\n",
            "216 218\n",
            "218 220\n",
            "220 222\n",
            "222 224\n",
            "224 226\n",
            "226 228\n",
            "228 230\n",
            "230 232\n",
            "232 234\n",
            "234 236\n",
            "236 238\n",
            "238 240\n",
            "240 242\n",
            "242 244\n",
            "244 246\n",
            "246 248\n",
            "248 250\n",
            "250 252\n",
            "252 254\n",
            "254 256\n",
            "256 258\n",
            "258 260\n",
            "260 262\n",
            "262 264\n",
            "264 266\n",
            "266 268\n",
            "268 270\n",
            "270 272\n",
            "272 274\n",
            "274 276\n",
            "276 278\n",
            "278 280\n",
            "280 282\n",
            "282 284\n",
            "284 286\n",
            "286 288\n",
            "288 290\n",
            "290 292\n",
            "292 294\n",
            "294 296\n",
            "296 298\n",
            "298 300\n",
            "300 302\n",
            "302 304\n",
            "304 306\n",
            "306 308\n",
            "308 310\n",
            "310 312\n",
            "312 314\n",
            "314 316\n",
            "316 318\n",
            "318 320\n",
            "320 322\n",
            "322 324\n",
            "324 326\n",
            "326 328\n",
            "328 330\n",
            "330 332\n",
            "332 334\n",
            "334 336\n",
            "336 338\n",
            "338 340\n",
            "340 342\n",
            "342 344\n",
            "344 346\n",
            "346 348\n",
            "348 350\n",
            "350 352\n",
            "352 354\n",
            "354 356\n",
            "356 358\n",
            "358 360\n",
            "360 362\n",
            "362 364\n",
            "364 366\n",
            "366 368\n",
            "368 370\n",
            "370 372\n",
            "372 374\n",
            "374 376\n",
            "376 378\n",
            "378 380\n",
            "380 382\n",
            "382 384\n",
            "384 386\n",
            "386 388\n",
            "388 390\n",
            "390 392\n",
            "392 394\n",
            "394 396\n",
            "396 398\n",
            "398 400\n",
            "400 402\n",
            "402 404\n",
            "404 406\n",
            "406 408\n",
            "408 410\n",
            "410 412\n",
            "412 414\n",
            "414 416\n",
            "416 418\n",
            "418 420\n",
            "420 422\n",
            "422 424\n",
            "424 426\n",
            "426 428\n",
            "428 430\n",
            "430 432\n",
            "432 434\n",
            "434 436\n",
            "436 438\n",
            "438 440\n",
            "440 442\n",
            "442 444\n",
            "444 446\n",
            "446 448\n",
            "448 450\n",
            "450 452\n",
            "452 454\n",
            "454 456\n",
            "456 458\n",
            "458 460\n",
            "460 462\n",
            "462 464\n",
            "464 466\n",
            "466 468\n",
            "468 470\n",
            "470 472\n",
            "472 474\n",
            "474 476\n",
            "476 478\n",
            "478 480\n",
            "480 482\n",
            "482 484\n",
            "484 486\n",
            "486 488\n",
            "488 490\n",
            "490 492\n",
            "492 494\n",
            "494 496\n",
            "496 498\n",
            "498 500\n",
            "500 502\n",
            "502 504\n",
            "504 506\n",
            "506 508\n",
            "508 510\n",
            "510 512\n",
            "512 514\n",
            "514 516\n",
            "516 518\n",
            "518 520\n",
            "520 522\n",
            "522 524\n",
            "524 526\n",
            "526 528\n",
            "528 530\n",
            "530 532\n",
            "532 534\n",
            "534 536\n",
            "536 538\n",
            "538 540\n",
            "540 542\n",
            "542 544\n",
            "544 546\n",
            "546 548\n",
            "548 550\n",
            "550 552\n",
            "552 554\n",
            "554 556\n",
            "556 558\n",
            "558 560\n",
            "560 562\n",
            "562 564\n",
            "564 566\n",
            "566 568\n",
            "568 570\n",
            "570 572\n",
            "572 574\n",
            "574 576\n",
            "576 578\n",
            "578 580\n",
            "580 582\n",
            "582 584\n",
            "584 586\n",
            "586 588\n",
            "588 590\n",
            "590 592\n",
            "592 594\n",
            "594 596\n",
            "596 598\n",
            "598 600\n",
            "600 602\n",
            "602 604\n",
            "604 606\n",
            "606 608\n",
            "608 610\n",
            "610 612\n",
            "612 614\n",
            "614 616\n",
            "616 618\n",
            "618 620\n",
            "620 622\n",
            "622 624\n",
            "624 626\n",
            "626 628\n",
            "628 630\n",
            "630 632\n",
            "632 634\n",
            "634 636\n",
            "636 638\n",
            "638 640\n",
            "640 642\n",
            "642 644\n",
            "644 646\n",
            "646 648\n",
            "648 650\n",
            "650 652\n",
            "652 654\n",
            "654 656\n",
            "656 658\n",
            "658 660\n",
            "660 662\n",
            "662 664\n",
            "664 666\n",
            "666 668\n",
            "668 670\n",
            "670 672\n",
            "672 674\n",
            "674 676\n",
            "676 678\n",
            "678 680\n",
            "680 682\n",
            "682 684\n",
            "684 686\n",
            "686 688\n",
            "688 690\n",
            "690 692\n",
            "692 694\n",
            "694 696\n",
            "696 698\n",
            "698 700\n",
            "700 702\n",
            "702 704\n",
            "704 706\n",
            "706 708\n",
            "708 710\n",
            "710 712\n",
            "712 714\n",
            "714 716\n",
            "716 718\n",
            "718 720\n",
            "720 722\n",
            "722 724\n",
            "724 726\n",
            "726 728\n",
            "728 730\n",
            "730 732\n",
            "732 734\n",
            "734 736\n",
            "736 738\n",
            "738 740\n",
            "740 742\n",
            "742 744\n",
            "744 746\n",
            "746 748\n",
            "748 750\n",
            "750 752\n",
            "752 754\n",
            "754 756\n",
            "756 758\n",
            "758 760\n",
            "760 762\n",
            "762 764\n",
            "764 766\n",
            "766 768\n",
            "768 770\n",
            "770 772\n",
            "772 774\n",
            "774 776\n",
            "776 778\n",
            "778 780\n",
            "780 782\n",
            "782 784\n",
            "784 786\n",
            "786 788\n",
            "788 790\n",
            "790 792\n",
            "792 794\n",
            "794 796\n",
            "796 798\n",
            "798 800\n",
            "800 802\n",
            "802 804\n",
            "804 806\n",
            "806 808\n",
            "808 810\n",
            "810 812\n",
            "812 814\n",
            "814 816\n",
            "816 818\n",
            "818 820\n",
            "820 822\n",
            "822 824\n",
            "824 826\n",
            "826 828\n",
            "828 830\n",
            "830 832\n",
            "832 834\n",
            "834 836\n",
            "836 838\n",
            "838 840\n",
            "840 842\n",
            "842 844\n",
            "844 846\n",
            "846 848\n",
            "848 850\n",
            "850 852\n",
            "852 854\n",
            "854 856\n",
            "856 858\n",
            "858 860\n",
            "860 862\n",
            "862 864\n",
            "864 866\n",
            "866 868\n",
            "868 870\n",
            "870 872\n",
            "872 874\n",
            "874 876\n",
            "876 878\n",
            "878 880\n",
            "880 882\n",
            "882 884\n",
            "884 886\n",
            "886 888\n",
            "888 890\n",
            "890 892\n",
            "892 894\n",
            "894 896\n",
            "896 898\n",
            "898 900\n",
            "900 902\n",
            "902 904\n",
            "904 906\n",
            "906 908\n",
            "908 910\n",
            "910 912\n",
            "912 914\n",
            "914 916\n",
            "916 918\n",
            "918 920\n",
            "920 922\n",
            "922 924\n",
            "924 926\n",
            "926 928\n",
            "928 930\n",
            "930 932\n",
            "932 934\n",
            "934 936\n",
            "936 938\n",
            "938 940\n",
            "940 942\n",
            "942 944\n",
            "944 946\n",
            "946 948\n",
            "948 950\n",
            "950 952\n",
            "952 954\n",
            "954 956\n",
            "956 958\n",
            "958 960\n",
            "960 962\n",
            "962 964\n",
            "964 966\n",
            "966 968\n",
            "968 970\n",
            "970 972\n",
            "972 974\n",
            "974 976\n",
            "976 978\n",
            "978 980\n",
            "980 982\n",
            "982 984\n",
            "984 986\n",
            "986 988\n",
            "988 990\n",
            "990 992\n",
            "992 994\n",
            "994 996\n",
            "996 998\n",
            "998 1000\n",
            "1000 1002\n",
            "1002 1004\n",
            "1004 1006\n",
            "1006 1008\n",
            "1008 1010\n",
            "1010 1012\n",
            "1012 1014\n",
            "1014 1016\n",
            "1016 1018\n",
            "1018 1020\n",
            "1020 1022\n",
            "1022 1024\n",
            "1024 1026\n",
            "1026 1028\n",
            "1028 1030\n",
            "1030 1032\n",
            "1032 1034\n",
            "1034 1036\n",
            "1036 1038\n",
            "1038 1040\n",
            "1040 1042\n",
            "1042 1044\n",
            "1044 1046\n",
            "1046 1048\n",
            "1048 1050\n",
            "1050 1052\n",
            "1052 1054\n",
            "1054 1056\n",
            "1056 1058\n",
            "1058 1060\n",
            "1060 1062\n",
            "1062 1064\n",
            "1064 1066\n",
            "1066 1068\n",
            "1068 1070\n",
            "1070 1072\n",
            "1072 1074\n",
            "1074 1076\n",
            "1076 1078\n",
            "1078 1080\n",
            "1080 1082\n",
            "1082 1084\n",
            "1084 1086\n",
            "1086 1088\n",
            "1088 1090\n",
            "1090 1092\n",
            "1092 1094\n",
            "1094 1096\n",
            "1096 1098\n",
            "1098 1100\n",
            "1100 1102\n",
            "1102 1104\n",
            "1104 1106\n",
            "1106 1108\n",
            "1108 1110\n",
            "1110 1112\n",
            "1112 1114\n",
            "1114 1116\n",
            "1116 1118\n",
            "1118 1120\n",
            "1120 1122\n",
            "1122 1124\n",
            "1124 1126\n",
            "1126 1128\n",
            "1128 1130\n",
            "1130 1132\n",
            "1132 1134\n",
            "1134 1136\n",
            "1136 1138\n",
            "1138 1140\n",
            "1140 1142\n",
            "1142 1144\n",
            "1144 1146\n",
            "1146 1148\n",
            "1148 1150\n",
            "1150 1152\n",
            "1152 1154\n",
            "1154 1156\n",
            "1156 1158\n",
            "1158 1160\n",
            "1160 1162\n",
            "1162 1164\n",
            "1164 1166\n",
            "1166 1168\n",
            "1168 1170\n",
            "1170 1172\n",
            "1172 1174\n",
            "1174 1176\n",
            "1176 1178\n",
            "1178 1180\n",
            "1180 1182\n",
            "1182 1184\n",
            "1184 1186\n",
            "1186 1188\n",
            "1188 1190\n",
            "1190 1192\n",
            "1192 1194\n",
            "1194 1196\n",
            "1196 1198\n",
            "1198 1200\n",
            "1200 1202\n",
            "1202 1204\n",
            "1204 1206\n",
            "1206 1208\n",
            "1208 1210\n",
            "1210 1212\n",
            "1212 1214\n",
            "1214 1216\n",
            "1216 1218\n",
            "1218 1220\n",
            "1220 1222\n",
            "1222 1224\n",
            "1224 1226\n",
            "1226 1228\n",
            "1228 1230\n",
            "1230 1232\n",
            "1232 1234\n",
            "1234 1236\n",
            "1236 1238\n",
            "1238 1240\n",
            "1240 1242\n",
            "1242 1244\n",
            "1244 1246\n",
            "1246 1248\n",
            "1248 1250\n",
            "1250 1252\n",
            "1252 1254\n",
            "1254 1256\n",
            "1256 1258\n",
            "1258 1260\n",
            "1260 1262\n",
            "1262 1264\n",
            "1264 1266\n",
            "1266 1268\n",
            "1268 1270\n",
            "1270 1272\n",
            "1272 1274\n",
            "1274 1276\n",
            "1276 1278\n",
            "1278 1280\n",
            "1280 1282\n",
            "1282 1284\n",
            "1284 1286\n",
            "1286 1288\n",
            "1288 1290\n",
            "1290 1292\n",
            "1292 1294\n",
            "1294 1296\n",
            "1296 1298\n",
            "1298 1300\n",
            "1300 1302\n",
            "1302 1304\n",
            "1304 1306\n",
            "1306 1308\n",
            "1308 1310\n",
            "1310 1312\n",
            "1312 1314\n",
            "1314 1316\n",
            "1316 1318\n",
            "1318 1320\n",
            "1320 1322\n",
            "1322 1324\n",
            "1324 1326\n",
            "1326 1328\n",
            "1328 1330\n",
            "1330 1332\n",
            "1332 1334\n",
            "1334 1336\n",
            "1336 1338\n",
            "1338 1340\n",
            "1340 1342\n",
            "1342 1344\n",
            "1344 1346\n",
            "1346 1348\n",
            "1348 1350\n",
            "1350 1352\n",
            "1352 1354\n",
            "1354 1356\n",
            "1356 1358\n",
            "1358 1360\n",
            "1360 1362\n",
            "1362 1364\n",
            "1364 1366\n",
            "1366 1368\n",
            "1368 1370\n",
            "1370 1372\n",
            "1372 1374\n",
            "1374 1376\n",
            "1376 1378\n",
            "1378 1380\n",
            "1380 1382\n",
            "1382 1384\n",
            "1384 1386\n",
            "1386 1388\n",
            "1388 1390\n",
            "1390 1392\n",
            "1392 1394\n",
            "1394 1396\n",
            "1396 1398\n",
            "1398 1400\n",
            "1400 1402\n",
            "1402 1404\n",
            "1404 1406\n",
            "1406 1408\n",
            "1408 1410\n",
            "1410 1412\n",
            "1412 1414\n",
            "1414 1416\n",
            "1416 1418\n",
            "1418 1420\n",
            "1420 1422\n",
            "1422 1424\n",
            "1424 1426\n",
            "1426 1428\n",
            "1428 1430\n",
            "1430 1432\n",
            "1432 1434\n",
            "1434 1436\n",
            "1436 1438\n",
            "1438 1440\n",
            "1440 1442\n",
            "1442 1444\n",
            "1444 1446\n",
            "1446 1448\n",
            "1448 1450\n",
            "1450 1452\n",
            "1452 1454\n",
            "1454 1456\n",
            "1456 1458\n",
            "1458 1460\n",
            "1460 1462\n",
            "1462 1464\n",
            "1464 1466\n",
            "1466 1468\n",
            "1468 1470\n",
            "1470 1472\n",
            "1472 1474\n",
            "1474 1476\n",
            "1476 1478\n",
            "1478 1480\n",
            "1480 1482\n",
            "1482 1484\n",
            "1484 1486\n",
            "1486 1488\n",
            "1488 1490\n",
            "1490 1492\n",
            "1492 1494\n",
            "1494 1496\n",
            "1496 1498\n",
            "1498 1500\n",
            "1500 1502\n",
            "1502 1504\n",
            "1504 1506\n",
            "1506 1508\n",
            "1508 1510\n",
            "1510 1512\n",
            "1512 1514\n",
            "1514 1516\n",
            "1516 1518\n",
            "1518 1520\n",
            "1520 1522\n",
            "1522 1524\n",
            "1524 1526\n",
            "1526 1528\n",
            "1528 1530\n",
            "1530 1532\n",
            "1532 1534\n",
            "1534 1536\n",
            "1536 1538\n",
            "1538 1540\n",
            "1540 1542\n",
            "1542 1544\n",
            "1544 1546\n",
            "1546 1548\n",
            "1548 1550\n",
            "1550 1552\n",
            "1552 1554\n",
            "1554 1556\n",
            "1556 1558\n",
            "1558 1560\n",
            "1560 1562\n",
            "1562 1564\n",
            "1564 1566\n",
            "1566 1568\n",
            "1568 1570\n",
            "1570 1572\n",
            "1572 1574\n",
            "1574 1576\n",
            "1576 1578\n",
            "1578 1580\n",
            "1580 1582\n",
            "1582 1584\n",
            "1584 1586\n",
            "1586 1588\n",
            "1588 1590\n",
            "1590 1592\n",
            "1592 1594\n",
            "1594 1596\n",
            "1596 1598\n",
            "1598 1600\n",
            "1600 1602\n",
            "1602 1604\n",
            "1604 1606\n",
            "1606 1608\n",
            "1608 1610\n",
            "1610 1612\n",
            "1612 1614\n",
            "1614 1616\n",
            "1616 1618\n",
            "1618 1620\n",
            "1620 1622\n",
            "1622 1624\n",
            "1624 1626\n",
            "1626 1628\n",
            "1628 1630\n",
            "1630 1632\n",
            "1632 1634\n",
            "1634 1636\n",
            "1636 1638\n",
            "1638 1640\n",
            "1640 1642\n",
            "1642 1644\n",
            "1644 1646\n",
            "1646 1648\n",
            "1648 1650\n",
            "1650 1652\n",
            "1652 1654\n",
            "1654 1656\n",
            "1656 1658\n",
            "1658 1660\n",
            "1660 1662\n",
            "1662 1664\n",
            "1664 1666\n",
            "1666 1668\n",
            "1668 1670\n",
            "1670 1672\n",
            "1672 1674\n",
            "1674 1676\n",
            "1676 1678\n",
            "1678 1680\n",
            "1680 1682\n",
            "1682 1684\n",
            "1684 1686\n",
            "1686 1688\n",
            "1688 1690\n",
            "1690 1692\n",
            "1692 1694\n",
            "1694 1696\n",
            "1696 1698\n",
            "1698 1700\n",
            "1700 1702\n",
            "1702 1704\n",
            "1704 1706\n",
            "1706 1708\n",
            "1708 1710\n",
            "1710 1712\n",
            "1712 1714\n",
            "1714 1716\n",
            "1716 1718\n",
            "1718 1720\n",
            "1720 1722\n",
            "1722 1724\n",
            "1724 1726\n",
            "1726 1728\n",
            "1728 1730\n",
            "1730 1732\n",
            "1732 1734\n",
            "1734 1736\n",
            "1736 1738\n",
            "1738 1740\n",
            "1740 1742\n",
            "1742 1744\n",
            "1744 1746\n",
            "1746 1748\n",
            "1748 1750\n",
            "1750 1752\n",
            "1752 1754\n",
            "1754 1756\n",
            "1756 1758\n",
            "1758 1760\n",
            "1760 1762\n",
            "1762 1764\n",
            "1764 1766\n",
            "1766 1768\n",
            "1768 1770\n",
            "1770 1772\n",
            "1772 1774\n",
            "1774 1776\n",
            "1776 1778\n",
            "1778 1780\n",
            "1780 1782\n",
            "1782 1784\n",
            "1784 1786\n",
            "1786 1788\n",
            "1788 1790\n",
            "1790 1792\n",
            "1792 1794\n",
            "1794 1796\n",
            "1796 1798\n",
            "1798 1800\n",
            "1800 1802\n",
            "1802 1804\n",
            "1804 1806\n",
            "1806 1808\n",
            "1808 1810\n",
            "1810 1812\n",
            "1812 1814\n",
            "1814 1816\n",
            "1816 1818\n",
            "1818 1820\n",
            "1820 1822\n",
            "1822 1824\n",
            "1824 1826\n",
            "1826 1828\n",
            "1828 1830\n",
            "1830 1832\n",
            "1832 1834\n",
            "1834 1836\n",
            "1836 1838\n",
            "1838 1840\n",
            "1840 1842\n",
            "1842 1844\n",
            "1844 1846\n",
            "1846 1848\n",
            "1848 1850\n",
            "1850 1852\n",
            "1852 1854\n",
            "1854 1856\n",
            "1856 1858\n",
            "1858 1860\n",
            "1860 1862\n",
            "1862 1864\n",
            "1864 1866\n",
            "1866 1868\n",
            "1868 1870\n",
            "1870 1872\n",
            "1872 1874\n",
            "1874 1876\n",
            "1876 1878\n",
            "1878 1880\n",
            "1880 1882\n",
            "1882 1884\n",
            "1884 1886\n",
            "1886 1888\n",
            "1888 1890\n",
            "1890 1892\n",
            "1892 1894\n",
            "1894 1896\n",
            "1896 1898\n",
            "1898 1900\n",
            "1900 1902\n",
            "1902 1904\n",
            "1904 1906\n",
            "1906 1908\n",
            "1908 1910\n",
            "1910 1912\n",
            "1912 1914\n",
            "1914 1916\n",
            "1916 1918\n",
            "1918 1920\n",
            "1920 1922\n",
            "1922 1924\n",
            "1924 1926\n",
            "1926 1928\n",
            "1928 1930\n",
            "1930 1932\n",
            "1932 1934\n",
            "1934 1936\n",
            "1936 1938\n",
            "1938 1940\n",
            "1940 1942\n",
            "1942 1944\n",
            "1944 1946\n",
            "1946 1948\n",
            "1948 1950\n",
            "1950 1952\n",
            "1952 1954\n",
            "1954 1956\n",
            "1956 1958\n",
            "1958 1960\n",
            "1960 1962\n",
            "1962 1964\n",
            "1964 1966\n",
            "1966 1968\n",
            "1968 1970\n",
            "1970 1972\n",
            "1972 1974\n",
            "1974 1976\n",
            "1976 1978\n",
            "1978 1980\n",
            "1980 1982\n",
            "1982 1984\n",
            "1984 1986\n",
            "1986 1988\n",
            "1988 1990\n",
            "1990 1992\n",
            "1992 1994\n",
            "1994 1996\n",
            "1996 1998\n",
            "1998 2000\n",
            "2000 2002\n",
            "2002 2004\n",
            "2004 2006\n",
            "2006 2008\n",
            "2008 2010\n",
            "2010 2012\n",
            "2012 2014\n",
            "2014 2016\n",
            "2016 2018\n",
            "2018 2020\n",
            "2020 2022\n",
            "2022 2024\n",
            "2024 2026\n",
            "2026 2028\n",
            "2028 2030\n",
            "2030 2032\n",
            "2032 2034\n",
            "2034 2036\n",
            "2036 2038\n",
            "2038 2040\n",
            "2040 2042\n",
            "2042 2044\n",
            "2044 2046\n",
            "2046 2048\n",
            "2048 2050\n",
            "2050 2052\n",
            "2052 2054\n",
            "2054 2056\n",
            "2056 2058\n",
            "2058 2060\n",
            "2060 2062\n",
            "2062 2064\n",
            "2064 2066\n",
            "2066 2068\n",
            "2068 2070\n",
            "2070 2072\n",
            "2072 2074\n",
            "2074 2076\n",
            "2076 2078\n",
            "2078 2080\n",
            "2080 2082\n",
            "2082 2084\n",
            "2084 2086\n",
            "2086 2088\n",
            "2088 2090\n",
            "2090 2092\n",
            "2092 2094\n",
            "2094 2096\n",
            "2096 2098\n",
            "2098 2100\n",
            "2100 2102\n",
            "2102 2104\n",
            "2104 2106\n",
            "2106 2108\n",
            "2108 2110\n",
            "2110 2112\n",
            "2112 2114\n",
            "2114 2116\n",
            "2116 2118\n",
            "2118 2120\n",
            "2120 2122\n",
            "2122 2124\n",
            "2124 2126\n",
            "2126 2128\n",
            "2128 2130\n",
            "2130 2132\n",
            "2132 2134\n",
            "2134 2136\n",
            "2136 2138\n",
            "2138 2140\n",
            "2140 2142\n",
            "2142 2144\n",
            "2144 2146\n",
            "2146 2148\n",
            "2148 2150\n",
            "2150 2152\n",
            "2152 2154\n",
            "2154 2156\n",
            "2156 2158\n",
            "2158 2160\n",
            "2160 2162\n",
            "2162 2164\n",
            "2164 2166\n",
            "2166 2168\n",
            "2168 2170\n",
            "2170 2172\n",
            "2172 2174\n",
            "2174 2176\n",
            "2176 2178\n",
            "2178 2180\n",
            "2180 2182\n",
            "2182 2184\n",
            "2184 2186\n",
            "2186 2188\n",
            "2188 2190\n",
            "2190 2192\n",
            "2192 2194\n",
            "2194 2196\n",
            "2196 2198\n",
            "2198 2200\n",
            "2200 2202\n",
            "2202 2204\n",
            "2204 2206\n",
            "2206 2208\n",
            "2208 2210\n",
            "2210 2212\n",
            "2212 2214\n",
            "2214 2216\n",
            "2216 2218\n",
            "2218 2220\n",
            "2220 2222\n",
            "2222 2224\n",
            "2224 2226\n",
            "2226 2228\n",
            "2228 2230\n",
            "2230 2232\n",
            "2232 2234\n",
            "2234 2236\n",
            "2236 2238\n",
            "2238 2240\n",
            "2240 2242\n",
            "2242 2244\n",
            "2244 2246\n",
            "2246 2248\n",
            "2248 2250\n",
            "2250 2252\n",
            "2252 2254\n",
            "2254 2256\n",
            "2256 2258\n",
            "2258 2260\n",
            "2260 2262\n",
            "2262 2264\n",
            "2264 2266\n",
            "2266 2268\n",
            "2268 2270\n",
            "2270 2272\n",
            "2272 2274\n",
            "2274 2276\n",
            "2276 2278\n",
            "2278 2280\n",
            "2280 2282\n",
            "2282 2284\n",
            "2284 2286\n",
            "2286 2288\n",
            "2288 2290\n",
            "2290 2292\n",
            "2292 2294\n",
            "2294 2296\n",
            "2296 2298\n",
            "2298 2300\n",
            "2300 2302\n",
            "2302 2304\n",
            "2304 2306\n",
            "2306 2308\n",
            "2308 2310\n",
            "2310 2312\n",
            "2312 2314\n",
            "2314 2316\n",
            "2316 2318\n",
            "2318 2320\n",
            "2320 2322\n",
            "2322 2324\n",
            "2324 2326\n",
            "2326 2328\n",
            "2328 2330\n",
            "2330 2332\n",
            "2332 2334\n",
            "2334 2336\n",
            "2336 2338\n",
            "2338 2340\n",
            "2340 2342\n",
            "2342 2344\n",
            "2344 2346\n",
            "2346 2348\n",
            "2348 2350\n",
            "2350 2352\n",
            "2352 2354\n",
            "2354 2356\n",
            "2356 2358\n",
            "2358 2360\n",
            "2360 2362\n",
            "2362 2364\n",
            "2364 2366\n",
            "2366 2368\n",
            "2368 2370\n",
            "2370 2372\n",
            "2372 2374\n",
            "2374 2376\n",
            "2376 2378\n",
            "2378 2380\n",
            "2380 2382\n",
            "2382 2384\n",
            "2384 2386\n",
            "2386 2388\n",
            "2388 2390\n",
            "2390 2392\n",
            "2392 2394\n",
            "2394 2396\n",
            "2396 2398\n",
            "2398 2400\n",
            "2400 2402\n",
            "2402 2404\n",
            "2404 2406\n",
            "2406 2408\n",
            "2408 2410\n",
            "2410 2412\n",
            "2412 2414\n",
            "2414 2416\n",
            "2416 2418\n",
            "2418 2420\n",
            "2420 2422\n",
            "2422 2424\n",
            "2424 2426\n",
            "2426 2428\n",
            "2428 2430\n",
            "2430 2432\n",
            "2432 2434\n",
            "2434 2436\n",
            "2436 2438\n",
            "2438 2440\n",
            "2440 2442\n",
            "2442 2444\n",
            "2444 2446\n",
            "2446 2448\n",
            "2448 2450\n",
            "2450 2452\n",
            "2452 2454\n",
            "2454 2456\n",
            "2456 2458\n",
            "2458 2460\n",
            "2460 2462\n",
            "2462 2464\n",
            "2464 2466\n",
            "2466 2468\n",
            "2468 2470\n",
            "2470 2472\n",
            "2472 2474\n",
            "2474 2476\n",
            "2476 2478\n",
            "2478 2480\n",
            "2480 2482\n",
            "2482 2484\n",
            "2484 2486\n",
            "2486 2488\n",
            "2488 2490\n",
            "2490 2492\n",
            "2492 2494\n",
            "2494 2496\n",
            "2496 2498\n",
            "2498 2500\n",
            "2500 2502\n",
            "2502 2504\n",
            "2504 2506\n",
            "2506 2508\n",
            "2508 2510\n",
            "2510 2512\n",
            "2512 2514\n",
            "2514 2516\n",
            "2516 2518\n",
            "2518 2520\n",
            "2520 2522\n",
            "2522 2524\n",
            "2524 2526\n",
            "2526 2528\n",
            "2528 2530\n",
            "2530 2532\n",
            "2532 2534\n",
            "2534 2536\n",
            "2536 2538\n",
            "2538 2540\n",
            "2540 2542\n",
            "2542 2544\n",
            "2544 2546\n",
            "2546 2548\n",
            "2548 2550\n",
            "2550 2552\n",
            "2552 2554\n",
            "2554 2556\n",
            "2556 2558\n",
            "2558 2560\n",
            "2560 2562\n",
            "2562 2564\n",
            "2564 2566\n",
            "2566 2568\n",
            "2568 2570\n",
            "2570 2572\n",
            "2572 2574\n",
            "2574 2576\n",
            "2576 2578\n",
            "2578 2580\n",
            "2580 2582\n",
            "2582 2584\n",
            "2584 2586\n",
            "2586 2588\n",
            "2588 2590\n",
            "2590 2592\n",
            "2592 2594\n",
            "2594 2596\n",
            "2596 2598\n",
            "2598 2600\n",
            "2600 2602\n",
            "2602 2604\n",
            "2604 2606\n",
            "2606 2608\n",
            "2608 2610\n",
            "2610 2612\n",
            "2612 2614\n",
            "2614 2616\n",
            "2616 2618\n",
            "2618 2620\n",
            "2620 2622\n",
            "2622 2624\n",
            "2624 2626\n",
            "2626 2628\n",
            "2628 2630\n",
            "2630 2632\n",
            "2632 2634\n",
            "2634 2636\n",
            "2636 2638\n",
            "2638 2640\n",
            "2640 2642\n",
            "2642 2644\n",
            "2644 2646\n",
            "2646 2648\n",
            "2648 2650\n",
            "2650 2652\n",
            "2652 2654\n",
            "2654 2656\n",
            "2656 2658\n",
            "2658 2660\n",
            "2660 2662\n",
            "2662 2664\n",
            "2664 2666\n",
            "2666 2668\n",
            "2668 2670\n",
            "2670 2672\n",
            "2672 2674\n",
            "2674 2676\n",
            "2676 2678\n",
            "2678 2680\n",
            "2680 2682\n",
            "2682 2684\n",
            "2684 2686\n",
            "2686 2688\n",
            "2688 2690\n",
            "2690 2692\n",
            "2692 2694\n",
            "2694 2696\n",
            "2696 2698\n",
            "2698 2700\n",
            "2700 2702\n",
            "2702 2704\n",
            "2704 2706\n",
            "2706 2708\n",
            "2708 2710\n",
            "2710 2712\n",
            "2712 2714\n",
            "2714 2716\n",
            "2716 2718\n",
            "2718 2720\n",
            "2720 2722\n",
            "2722 2724\n",
            "2724 2726\n",
            "2726 2728\n",
            "2728 2730\n",
            "2730 2732\n",
            "2732 2734\n",
            "2734 2736\n",
            "2736 2738\n",
            "2738 2740\n",
            "2740 2742\n",
            "2742 2744\n",
            "2744 2746\n",
            "2746 2748\n",
            "2748 2750\n",
            "2750 2752\n",
            "2752 2754\n",
            "2754 2756\n",
            "2756 2758\n",
            "2758 2760\n",
            "2760 2762\n",
            "2762 2764\n",
            "2764 2766\n",
            "2766 2768\n",
            "2768 2770\n",
            "2770 2772\n",
            "2772 2774\n",
            "2774 2776\n",
            "2776 2778\n",
            "2778 2780\n",
            "2780 2782\n",
            "2782 2784\n",
            "2784 2786\n",
            "2786 2788\n",
            "2788 2790\n",
            "2790 2792\n",
            "2792 2794\n",
            "2794 2796\n",
            "2796 2798\n",
            "2798 2800\n",
            "2800 2802\n",
            "2802 2804\n",
            "2804 2806\n",
            "2806 2808\n",
            "2808 2810\n",
            "2810 2812\n",
            "2812 2814\n",
            "2814 2816\n",
            "2816 2818\n",
            "2818 2820\n",
            "2820 2822\n",
            "2822 2824\n",
            "2824 2826\n",
            "2826 2828\n",
            "2828 2830\n",
            "2830 2832\n",
            "2832 2834\n",
            "2834 2836\n",
            "2836 2838\n",
            "2838 2840\n",
            "2840 2842\n",
            "2842 2844\n",
            "2844 2846\n",
            "2846 2848\n",
            "2848 2850\n",
            "2850 2852\n",
            "2852 2854\n",
            "2854 2856\n",
            "2856 2858\n",
            "2858 2860\n",
            "2860 2862\n",
            "2862 2864\n",
            "2864 2866\n",
            "2866 2868\n",
            "2868 2870\n",
            "2870 2872\n",
            "2872 2874\n",
            "2874 2876\n",
            "2876 2878\n",
            "2878 2880\n",
            "2880 2882\n",
            "2882 2884\n",
            "2884 2886\n",
            "2886 2888\n",
            "2888 2890\n",
            "2890 2892\n",
            "2892 2894\n",
            "2894 2896\n",
            "2896 2898\n",
            "2898 2900\n",
            "2900 2902\n",
            "2902 2904\n",
            "2904 2906\n",
            "2906 2908\n",
            "2908 2910\n",
            "2910 2912\n",
            "2912 2914\n",
            "2914 2916\n",
            "2916 2918\n",
            "2918 2920\n",
            "2920 2922\n",
            "2922 2924\n",
            "2924 2926\n",
            "2926 2928\n",
            "2928 2930\n",
            "2930 2932\n",
            "2932 2934\n",
            "2934 2936\n",
            "2936 2938\n",
            "2938 2940\n",
            "2940 2942\n",
            "2942 2944\n",
            "2944 2946\n",
            "2946 2948\n",
            "2948 2950\n",
            "2950 2952\n",
            "2952 2954\n",
            "2954 2956\n",
            "2956 2958\n",
            "2958 2960\n",
            "2960 2962\n",
            "2962 2964\n",
            "2964 2966\n",
            "2966 2968\n",
            "2968 2970\n",
            "2970 2972\n",
            "2972 2974\n",
            "2974 2976\n",
            "2976 2978\n",
            "2978 2980\n",
            "2980 2982\n",
            "2982 2984\n",
            "2984 2986\n",
            "2986 2988\n",
            "2988 2990\n",
            "2990 2992\n",
            "2992 2994\n",
            "2994 2996\n",
            "2996 2998\n",
            "2998 3000\n",
            "3000 3002\n",
            "3002 3004\n",
            "3004 3006\n",
            "3006 3008\n",
            "3008 3010\n",
            "3010 3012\n",
            "3012 3014\n",
            "3014 3016\n",
            "3016 3018\n",
            "3018 3020\n",
            "3020 3022\n",
            "3022 3024\n",
            "3024 3026\n",
            "3026 3028\n",
            "3028 3030\n",
            "3030 3032\n",
            "3032 3034\n",
            "3034 3036\n",
            "3036 3038\n",
            "3038 3040\n",
            "3040 3042\n",
            "3042 3044\n",
            "3044 3046\n",
            "3046 3048\n",
            "3048 3050\n",
            "3050 3052\n",
            "3052 3054\n",
            "3054 3056\n",
            "3056 3058\n",
            "3058 3060\n",
            "3060 3062\n",
            "3062 3064\n",
            "3064 3066\n",
            "3066 3068\n",
            "3068 3070\n",
            "3070 3072\n",
            "3072 3074\n",
            "3074 3076\n",
            "3076 3078\n",
            "3078 3080\n",
            "3080 3082\n",
            "3082 3084\n",
            "3084 3086\n",
            "3086 3088\n",
            "3088 3090\n",
            "3090 3092\n",
            "3092 3094\n",
            "3094 3096\n",
            "3096 3098\n",
            "3098 3100\n",
            "3100 3102\n",
            "3102 3104\n",
            "3104 3106\n",
            "3106 3108\n",
            "3108 3110\n",
            "3110 3112\n",
            "3112 3114\n",
            "3114 3116\n",
            "3116 3118\n",
            "3118 3120\n",
            "3120 3122\n",
            "3122 3124\n",
            "3124 3126\n",
            "3126 3128\n",
            "3128 3130\n",
            "3130 3132\n",
            "3132 3134\n",
            "3134 3136\n",
            "3136 3138\n",
            "3138 3140\n",
            "3140 3142\n",
            "3142 3144\n",
            "3144 3146\n",
            "3146 3148\n",
            "3148 3150\n",
            "3150 3152\n",
            "3152 3154\n",
            "3154 3156\n",
            "3156 3158\n",
            "3158 3160\n",
            "3160 3162\n",
            "3162 3164\n",
            "3164 3166\n",
            "3166 3168\n",
            "3168 3170\n",
            "3170 3172\n",
            "3172 3174\n",
            "3174 3176\n",
            "3176 3178\n",
            "3178 3180\n",
            "3180 3182\n",
            "3182 3184\n",
            "3184 3186\n",
            "3186 3188\n",
            "3188 3190\n",
            "3190 3192\n",
            "3192 3194\n",
            "3194 3196\n",
            "3196 3198\n",
            "3198 3200\n",
            "3200 3202\n",
            "3202 3204\n",
            "3204 3206\n",
            "3206 3208\n",
            "3208 3210\n",
            "3210 3212\n",
            "3212 3214\n",
            "3214 3216\n",
            "3216 3218\n",
            "3218 3220\n",
            "3220 3222\n",
            "3222 3224\n",
            "3224 3226\n",
            "3226 3228\n",
            "3228 3230\n",
            "3230 3232\n",
            "3232 3234\n",
            "3234 3236\n",
            "3236 3238\n",
            "3238 3240\n",
            "3240 3242\n",
            "3242 3244\n",
            "3244 3246\n",
            "3246 3248\n",
            "3248 3250\n",
            "3250 3252\n",
            "3252 3254\n",
            "3254 3256\n",
            "3256 3258\n",
            "3258 3260\n",
            "3260 3262\n",
            "3262 3264\n",
            "3264 3266\n",
            "3266 3268\n",
            "3268 3270\n",
            "3270 3272\n",
            "3272 3274\n",
            "3274 3276\n",
            "3276 3278\n",
            "3278 3280\n",
            "3280 3282\n",
            "3282 3284\n",
            "3284 3286\n",
            "3286 3288\n",
            "3288 3290\n",
            "3290 3292\n",
            "3292 3294\n",
            "3294 3296\n",
            "3296 3298\n",
            "3298 3300\n",
            "3300 3302\n",
            "3302 3304\n",
            "3304 3306\n",
            "3306 3308\n",
            "3308 3310\n",
            "3310 3312\n",
            "3312 3314\n",
            "3314 3316\n",
            "3316 3318\n",
            "3318 3320\n",
            "3320 3322\n",
            "3322 3324\n",
            "3324 3326\n",
            "3326 3328\n",
            "3328 3330\n",
            "3330 3332\n",
            "3332 3334\n",
            "3334 3336\n",
            "3336 3338\n",
            "3338 3340\n",
            "3340 3342\n",
            "3342 3344\n",
            "3344 3346\n",
            "3346 3348\n",
            "3348 3350\n",
            "3350 3352\n",
            "3352 3354\n",
            "3354 3356\n",
            "3356 3358\n",
            "3358 3360\n",
            "3360 3362\n",
            "3362 3364\n",
            "3364 3366\n",
            "3366 3368\n",
            "3368 3370\n",
            "3370 3372\n",
            "3372 3374\n",
            "3374 3376\n",
            "3376 3378\n"
          ]
        }
      ],
      "source": [
        "ibgs = []\n",
        "flow_p10PktSize  = []\n",
        "flow_p50PktSize = []\n",
        "flow_p90PktSize =  []\n",
        "rate = []\n",
        "flow_ipg_var = []\n",
        "flow_packet_size = []\n",
        "flow_packets = []\n",
        "flow_type = []\n",
        "flow_pkts_per_burst = []\n",
        "flow_bitrate_per_burst = []\n",
        "\n",
        "flow_ipg_mean = []\n",
        "flow_traffic = []\n",
        "link_capacity = []\n",
        "flow_length = []\n",
        "max_link_load = []\n",
        "path_to_link = []\n",
        "link_to_path = []\n",
        "\n",
        "delays =  []\n",
        "\n",
        "new_data =[]\n",
        "z_start = 0\n",
        "for z in range(16, len(train_list[:]),16):\n",
        "  ibgs = []\n",
        "  flow_p10PktSize  = []\n",
        "  flow_p50PktSize = []\n",
        "  flow_p90PktSize =  []\n",
        "  rate = []\n",
        "  flow_ipg_var = []\n",
        "  flow_packet_size = []\n",
        "  flow_packets = []\n",
        "  flow_type = []\n",
        "  flow_pkts_per_burst = []\n",
        "  flow_ipg_mean = []\n",
        "  flow_traffic = []\n",
        "  link_capacity = []\n",
        "  flow_length = []\n",
        "  max_link_load = []\n",
        "  path_to_link = []\n",
        "  link_to_path = []\n",
        "  delays =  []\n",
        "  flow_bitrate_per_burst = []\n",
        "\n",
        "\n",
        "  print(z_start, z)\n",
        "  i = 0\n",
        "  for train_lis in train_list[z_start:z]:\n",
        "\n",
        "\n",
        "\n",
        "      ibgs.append(train_lis[0][\"ibg\"])\n",
        "      flow_p10PktSize.append(np.array(train_lis[0][\"flow_p10PktSize\"]))\n",
        "      flow_p50PktSize.append(np.array(train_lis[0][\"flow_p50PktSize\"]))\n",
        "      flow_p90PktSize.append(np.array(train_lis[0][\"flow_p90PktSize\"]))\n",
        "      rate.append(np.array(train_lis[0][\"rate\"]))\n",
        "      flow_ipg_var.append(np.array(train_lis[0][\"flow_ipg_var\"]))\n",
        "      flow_packet_size.append(np.array(train_lis[0][\"flow_packet_size\"]))\n",
        "      flow_packets.append(np.array(train_lis[0][\"flow_packets\"]))\n",
        "      flow_type.append(np.array(train_lis[0][\"flow_type\"]))\n",
        "      flow_pkts_per_burst.append(np.array(train_lis[0][\"flow_pkts_per_burst\"]))\n",
        "      flow_ipg_mean.append(np.array(train_lis[0][\"flow_ipg_mean\"]))\n",
        "      flow_traffic.append(np.array(train_lis[0][\"flow_traffic\"]))\n",
        "      flow_bitrate_per_burst.append(np.array(train_lis[0][\"flow_bitrate_per_burst\"]))\n",
        "\n",
        "      link_capacity.append(np.array(train_lis[0][\"link_capacity\"]))\n",
        "      flow_length.append(np.array(train_lis[0][\"flow_length\"]))\n",
        "\n",
        "      # max_link_load.append(np.full(train_lis[0][\"flow_traffic\"].shape, np.array(train_lis[0][\"max_link_load\"]), ))\n",
        "      max_link_load.append(np.array(train_lis[0][\"max_link_load\"]) )\n",
        "\n",
        "      # path_to_link.append(np.array(train_lis[0][\"path_to_link\"]))\n",
        "\n",
        "\n",
        "      path_to_link_ = train_lis[0][\"path_to_link\"].numpy()\n",
        "\n",
        "      if i >0:\n",
        "        # train_lis[0][\"path_to_link\"] = np.array(train_lis[0][\"path_to_link\"])[:]\n",
        "\n",
        "        for path_to_lin in path_to_link_:\n",
        "            path_to_lin[:,0] = path_to_lin[:,0]+ np.array(flow_traffic[-2]).shape[0]\n",
        "\n",
        "\n",
        "        path_to_link.extend(path_to_link_)\n",
        "\n",
        "        link_to_path.extend(  list(np.array(train_lis[0][\"link_to_path\"]) +  link_capacity[-2].shape[0]))\n",
        "        # print(i, \"second iteration\", np.concatenate(link_capacity[-1]).shape[0], np.max(np.concatenate(link_to_path, axis =0)))\n",
        "      else:\n",
        "\n",
        "\n",
        "        link_to_path.extend( list(np.array(train_lis[0][\"link_to_path\"].numpy())))\n",
        "        path_to_link.extend(list(path_to_link_))\n",
        "        # print(i, \"first iteration\",  np.concatenate(link_capacity[-1]).shape[0], np.max(np.concatenate(link_to_path, axis =0)))\n",
        "      delays.append(np.array(train_lis[1]))\n",
        "      i+=1\n",
        "      # print(train_lis[0][\"devices\"].shape\n",
        "      # devices.append(train_lis[0][\"devices\"])\n",
        "\n",
        "  z_start = z\n",
        "\n",
        "  flow_traffic = np.concatenate(flow_traffic)\n",
        "  flow_p10PktSize = np.concatenate(flow_p10PktSize)\n",
        "  flow_p50PktSize = np.concatenate(flow_p50PktSize)\n",
        "  flow_p90PktSize = np.concatenate(flow_p90PktSize)\n",
        "  flow_pkts_per_burst = np.concatenate(flow_pkts_per_burst)\n",
        "  flow_ipg_mean = np.concatenate(flow_ipg_mean)\n",
        "  flow_ipg_var = np.concatenate(flow_ipg_var)\n",
        "  ibgs = np.concatenate(ibgs)\n",
        "  flow_length = np.concatenate(flow_length)\n",
        "  link_capacity = np.concatenate(link_capacity)\n",
        "  rate= np.concatenate(rate)\n",
        "  flow_packet_size = np.concatenate(flow_packet_size)\n",
        "  flow_packets = np.concatenate(flow_packets)\n",
        "  flow_type = np.concatenate(flow_type)\n",
        "  delays = np.concatenate(delays)\n",
        "  flow_bitrate_per_burst = np.concatenate(flow_bitrate_per_burst)\n",
        "  max_link_load = np.concatenate(max_link_load)\n",
        "  loads = []\n",
        "  normal_loads = []\n",
        " \n",
        " \n",
        "\n",
        "  step_wise = len(path_to_link)/ max_link_load.shape[0]\n",
        "  stepping = 0\n",
        "\n",
        "  for m in range(len(path_to_link)):\n",
        "      # print(\"m\", torch.tensor(path_to_link[m][:, 0 ]).max(), len(flow_traffic))\n",
        "      loads.append(torch.sum(torch.index_select((torch.tensor(flow_traffic)), 0, torch.tensor(path_to_link[m][:, 0 ]).type(torch.int64))) / (link_capacity[m] * 1e9))\n",
        "      if m>0 and m % step_wise == 0:\n",
        "        #  print(\"changing now\", m  )\n",
        "         stepping += 1\n",
        "      normal_loads.append(loads[-1]/max_link_load[stepping])\n",
        "\n",
        "  loads = torch.tensor(loads)\n",
        "  normal_load = torch.tensor(normal_loads)\n",
        " \n",
        "\n",
        "  # print(loads.shape,normal_load.shape )\n",
        "  loads_capacities = torch.cat(((loads).unsqueeze(1), normal_load.unsqueeze(1), torch.tensor(link_capacity)), dim=1)\n",
        "\n",
        "\n",
        "  width = flow_length.max()\n",
        "  b = []\n",
        "\n",
        "  for i in range(len(link_to_path)):\n",
        "      if len(link_to_path[i]) != width:\n",
        "          x = np.pad(link_to_path[i]+1, (0, width-len(link_to_path[i])), 'constant', constant_values=0)\n",
        "      else:\n",
        "          x = link_to_path[i]+1\n",
        "      b.append(x)\n",
        "  b = np.array(b)\n",
        "  tensor_list = [torch.from_numpy(seq) for seq in path_to_link]\n",
        "  tensor_lens = [len(seq) for seq in path_to_link]\n",
        "\n",
        "  new_data.append(((flow_traffic, flow_p10PktSize,flow_p90PktSize, flow_pkts_per_burst, flow_ipg_mean, flow_ipg_var, ibgs, flow_length, link_capacity,rate, flow_packet_size,flow_packets,flow_type, link_to_path, path_to_link, max_link_load, flow_bitrate_per_burst, loads, tensor_lens, tensor_list, b, loads_capacities), delays))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 2\n",
            "2 4\n",
            "4 6\n",
            "6 8\n",
            "8 10\n",
            "10 12\n",
            "12 14\n",
            "14 16\n",
            "16 18\n",
            "18 20\n",
            "20 22\n",
            "22 24\n",
            "24 26\n",
            "26 28\n",
            "28 30\n",
            "30 32\n",
            "32 34\n",
            "34 36\n",
            "36 38\n",
            "38 40\n",
            "40 42\n",
            "42 44\n",
            "44 46\n",
            "46 48\n",
            "48 50\n",
            "50 52\n",
            "52 54\n",
            "54 56\n",
            "56 58\n",
            "58 60\n",
            "60 62\n",
            "62 64\n",
            "64 66\n",
            "66 68\n",
            "68 70\n",
            "70 72\n",
            "72 74\n",
            "74 76\n",
            "76 78\n",
            "78 80\n",
            "80 82\n",
            "82 84\n",
            "84 86\n",
            "86 88\n",
            "88 90\n",
            "90 92\n",
            "92 94\n",
            "94 96\n",
            "96 98\n",
            "98 100\n",
            "100 102\n",
            "102 104\n",
            "104 106\n",
            "106 108\n",
            "108 110\n",
            "110 112\n",
            "112 114\n",
            "114 116\n",
            "116 118\n",
            "118 120\n",
            "120 122\n",
            "122 124\n",
            "124 126\n",
            "126 128\n",
            "128 130\n",
            "130 132\n",
            "132 134\n",
            "134 136\n",
            "136 138\n",
            "138 140\n",
            "140 142\n",
            "142 144\n",
            "144 146\n",
            "146 148\n",
            "148 150\n",
            "150 152\n",
            "152 154\n",
            "154 156\n",
            "156 158\n",
            "158 160\n",
            "160 162\n",
            "162 164\n",
            "164 166\n",
            "166 168\n",
            "168 170\n",
            "170 172\n",
            "172 174\n",
            "174 176\n",
            "176 178\n",
            "178 180\n",
            "180 182\n",
            "182 184\n",
            "184 186\n",
            "186 188\n",
            "188 190\n",
            "190 192\n",
            "192 194\n",
            "194 196\n",
            "196 198\n",
            "198 200\n",
            "200 202\n",
            "202 204\n",
            "204 206\n",
            "206 208\n",
            "208 210\n",
            "210 212\n",
            "212 214\n",
            "214 216\n",
            "216 218\n",
            "218 220\n",
            "220 222\n",
            "222 224\n",
            "224 226\n",
            "226 228\n",
            "228 230\n",
            "230 232\n",
            "232 234\n",
            "234 236\n",
            "236 238\n",
            "238 240\n",
            "240 242\n",
            "242 244\n",
            "244 246\n",
            "246 248\n",
            "248 250\n",
            "250 252\n",
            "252 254\n",
            "254 256\n",
            "256 258\n",
            "258 260\n",
            "260 262\n",
            "262 264\n",
            "264 266\n",
            "266 268\n",
            "268 270\n",
            "270 272\n",
            "272 274\n",
            "274 276\n",
            "276 278\n",
            "278 280\n",
            "280 282\n",
            "282 284\n",
            "284 286\n",
            "286 288\n",
            "288 290\n",
            "290 292\n",
            "292 294\n",
            "294 296\n",
            "296 298\n",
            "298 300\n",
            "300 302\n",
            "302 304\n",
            "304 306\n",
            "306 308\n",
            "308 310\n",
            "310 312\n",
            "312 314\n",
            "314 316\n",
            "316 318\n",
            "318 320\n",
            "320 322\n",
            "322 324\n",
            "324 326\n",
            "326 328\n",
            "328 330\n",
            "330 332\n",
            "332 334\n",
            "334 336\n",
            "336 338\n",
            "338 340\n",
            "340 342\n",
            "342 344\n",
            "344 346\n",
            "346 348\n",
            "348 350\n",
            "350 352\n",
            "352 354\n",
            "354 356\n",
            "356 358\n",
            "358 360\n",
            "360 362\n",
            "362 364\n",
            "364 366\n",
            "366 368\n",
            "368 370\n",
            "370 372\n",
            "372 374\n",
            "374 376\n",
            "376 378\n",
            "378 380\n",
            "380 382\n",
            "382 384\n",
            "384 386\n",
            "386 388\n",
            "388 390\n",
            "390 392\n",
            "392 394\n",
            "394 396\n",
            "396 398\n",
            "398 400\n",
            "400 402\n",
            "402 404\n",
            "404 406\n",
            "406 408\n",
            "408 410\n",
            "410 412\n",
            "412 414\n",
            "414 416\n",
            "416 418\n",
            "418 420\n",
            "420 422\n",
            "422 424\n",
            "424 426\n",
            "426 428\n",
            "428 430\n",
            "430 432\n",
            "432 434\n",
            "434 436\n",
            "436 438\n",
            "438 440\n",
            "440 442\n",
            "442 444\n",
            "444 446\n",
            "446 448\n",
            "448 450\n",
            "450 452\n",
            "452 454\n",
            "454 456\n",
            "456 458\n",
            "458 460\n",
            "460 462\n",
            "462 464\n",
            "464 466\n",
            "466 468\n",
            "468 470\n",
            "470 472\n",
            "472 474\n",
            "474 476\n",
            "476 478\n",
            "478 480\n",
            "480 482\n",
            "482 484\n",
            "484 486\n",
            "486 488\n",
            "488 490\n",
            "490 492\n",
            "492 494\n",
            "494 496\n",
            "496 498\n",
            "498 500\n",
            "500 502\n",
            "502 504\n",
            "504 506\n",
            "506 508\n",
            "508 510\n",
            "510 512\n",
            "512 514\n",
            "514 516\n",
            "516 518\n",
            "518 520\n",
            "520 522\n",
            "522 524\n",
            "524 526\n",
            "526 528\n",
            "528 530\n",
            "530 532\n",
            "532 534\n",
            "534 536\n",
            "536 538\n",
            "538 540\n",
            "540 542\n",
            "542 544\n",
            "544 546\n",
            "546 548\n",
            "548 550\n",
            "550 552\n",
            "552 554\n",
            "554 556\n",
            "556 558\n",
            "558 560\n",
            "560 562\n",
            "562 564\n",
            "564 566\n",
            "566 568\n",
            "568 570\n",
            "570 572\n",
            "572 574\n",
            "574 576\n",
            "576 578\n",
            "578 580\n",
            "580 582\n",
            "582 584\n",
            "584 586\n",
            "586 588\n",
            "588 590\n",
            "590 592\n",
            "592 594\n",
            "594 596\n",
            "596 598\n",
            "598 600\n",
            "600 602\n",
            "602 604\n",
            "604 606\n",
            "606 608\n",
            "608 610\n",
            "610 612\n",
            "612 614\n",
            "614 616\n",
            "616 618\n",
            "618 620\n",
            "620 622\n",
            "622 624\n",
            "624 626\n",
            "626 628\n",
            "628 630\n",
            "630 632\n",
            "632 634\n",
            "634 636\n",
            "636 638\n",
            "638 640\n",
            "640 642\n",
            "642 644\n",
            "644 646\n",
            "646 648\n",
            "648 650\n",
            "650 652\n",
            "652 654\n",
            "654 656\n",
            "656 658\n",
            "658 660\n",
            "660 662\n",
            "662 664\n",
            "664 666\n",
            "666 668\n",
            "668 670\n",
            "670 672\n",
            "672 674\n",
            "674 676\n",
            "676 678\n",
            "678 680\n",
            "680 682\n",
            "682 684\n",
            "684 686\n",
            "686 688\n",
            "688 690\n",
            "690 692\n",
            "692 694\n",
            "694 696\n",
            "696 698\n",
            "698 700\n",
            "700 702\n",
            "702 704\n",
            "704 706\n",
            "706 708\n",
            "708 710\n",
            "710 712\n",
            "712 714\n",
            "714 716\n",
            "716 718\n",
            "718 720\n",
            "720 722\n",
            "722 724\n",
            "724 726\n",
            "726 728\n",
            "728 730\n",
            "730 732\n",
            "732 734\n",
            "734 736\n",
            "736 738\n",
            "738 740\n",
            "740 742\n",
            "742 744\n",
            "744 746\n",
            "746 748\n",
            "748 750\n",
            "750 752\n",
            "752 754\n",
            "754 756\n",
            "756 758\n",
            "758 760\n",
            "760 762\n",
            "762 764\n",
            "764 766\n",
            "766 768\n",
            "768 770\n",
            "770 772\n",
            "772 774\n",
            "774 776\n",
            "776 778\n",
            "778 780\n",
            "780 782\n",
            "782 784\n",
            "784 786\n",
            "786 788\n",
            "788 790\n",
            "790 792\n",
            "792 794\n",
            "794 796\n",
            "796 798\n",
            "798 800\n",
            "800 802\n",
            "802 804\n",
            "804 806\n",
            "806 808\n",
            "808 810\n",
            "810 812\n",
            "812 814\n",
            "814 816\n",
            "816 818\n",
            "818 820\n",
            "820 822\n",
            "822 824\n",
            "824 826\n",
            "826 828\n",
            "828 830\n",
            "830 832\n",
            "832 834\n",
            "834 836\n",
            "836 838\n",
            "838 840\n",
            "840 842\n",
            "842 844\n"
          ]
        }
      ],
      "source": [
        "ibgs = []\n",
        "flow_p10PktSize  = []\n",
        "flow_p50PktSize = []\n",
        "flow_p90PktSize =  []\n",
        "rate = []\n",
        "flow_ipg_var = []\n",
        "flow_packet_size = []\n",
        "flow_packets = []\n",
        "flow_type = []\n",
        "flow_pkts_per_burst = []\n",
        "flow_bitrate_per_burst = []\n",
        "\n",
        "flow_ipg_mean = []\n",
        "flow_traffic = []\n",
        "link_capacity = []\n",
        "flow_length = []\n",
        "max_link_load = []\n",
        "path_to_link = []\n",
        "link_to_path = []\n",
        "\n",
        "delays =  []\n",
        "batch_size = 512\n",
        "# devices = []\n",
        "\n",
        "new_data_val =[]\n",
        "z_start = 0\n",
        "for z in range(16, len(val_list[:]),16):\n",
        "  ibgs = []\n",
        "  flow_p10PktSize  = []\n",
        "  flow_p50PktSize = []\n",
        "  flow_p90PktSize =  []\n",
        "  rate = []\n",
        "  flow_ipg_var = []\n",
        "  flow_packet_size = []\n",
        "  flow_packets = []\n",
        "  flow_type = []\n",
        "  flow_pkts_per_burst = []\n",
        "  flow_ipg_mean = []\n",
        "  flow_traffic = []\n",
        "  link_capacity = []\n",
        "  flow_length = []\n",
        "  max_link_load = []\n",
        "  path_to_link = []\n",
        "  link_to_path = []\n",
        "  delays =  []\n",
        "  flow_bitrate_per_burst = []\n",
        "\n",
        "\n",
        "  print(z_start, z)\n",
        "  i = 0\n",
        "  for train_lis in train_list[z_start:z]:\n",
        "\n",
        "\n",
        "\n",
        "      ibgs.append(train_lis[0][\"ibg\"])\n",
        "      flow_p10PktSize.append(np.array(train_lis[0][\"flow_p10PktSize\"]))\n",
        "      flow_p50PktSize.append(np.array(train_lis[0][\"flow_p50PktSize\"]))\n",
        "      flow_p90PktSize.append(np.array(train_lis[0][\"flow_p90PktSize\"]))\n",
        "      rate.append(np.array(train_lis[0][\"rate\"]))\n",
        "      flow_ipg_var.append(np.array(train_lis[0][\"flow_ipg_var\"]))\n",
        "      flow_packet_size.append(np.array(train_lis[0][\"flow_packet_size\"]))\n",
        "      flow_packets.append(np.array(train_lis[0][\"flow_packets\"]))\n",
        "      flow_type.append(np.array(train_lis[0][\"flow_type\"]))\n",
        "      flow_pkts_per_burst.append(np.array(train_lis[0][\"flow_pkts_per_burst\"]))\n",
        "      flow_ipg_mean.append(np.array(train_lis[0][\"flow_ipg_mean\"]))\n",
        "      flow_traffic.append(np.array(train_lis[0][\"flow_traffic\"]))\n",
        "      flow_bitrate_per_burst.append(np.array(train_lis[0][\"flow_bitrate_per_burst\"]))\n",
        "\n",
        "      link_capacity.append(np.array(train_lis[0][\"link_capacity\"]))\n",
        "      flow_length.append(np.array(train_lis[0][\"flow_length\"]))\n",
        "\n",
        "      # max_link_load.append(np.full(train_lis[0][\"flow_traffic\"].shape, np.array(train_lis[0][\"max_link_load\"]), ))\n",
        "      max_link_load.append(np.array(train_lis[0][\"max_link_load\"]) )\n",
        "\n",
        "      # path_to_link.append(np.array(train_lis[0][\"path_to_link\"]))\n",
        "\n",
        "\n",
        "      path_to_link_ = train_lis[0][\"path_to_link\"].numpy()\n",
        "\n",
        "      if i >0:\n",
        "        # train_lis[0][\"path_to_link\"] = np.array(train_lis[0][\"path_to_link\"])[:]\n",
        "\n",
        "        for path_to_lin in path_to_link_:\n",
        "            path_to_lin[:,0] = path_to_lin[:,0]+ np.array(flow_traffic[-2]).shape[0]\n",
        "\n",
        "\n",
        "        path_to_link.extend(path_to_link_)\n",
        "\n",
        "        link_to_path.extend(  list(np.array(train_lis[0][\"link_to_path\"]) +  link_capacity[-2].shape[0]))\n",
        "        # print(i, \"second iteration\", np.concatenate(link_capacity[-1]).shape[0], np.max(np.concatenate(link_to_path, axis =0)))\n",
        "      else:\n",
        "\n",
        "\n",
        "        link_to_path.extend( list(np.array(train_lis[0][\"link_to_path\"].numpy())))\n",
        "        path_to_link.extend(list(path_to_link_))\n",
        "        # print(i, \"first iteration\",  np.concatenate(link_capacity[-1]).shape[0], np.max(np.concatenate(link_to_path, axis =0)))\n",
        "      delays.append(np.array(train_lis[1]))\n",
        "      i+=1\n",
        "      # print(train_lis[0][\"devices\"].shape\n",
        "      # devices.append(train_lis[0][\"devices\"])\n",
        "\n",
        "  z_start = z\n",
        "\n",
        "  flow_traffic = np.concatenate(flow_traffic)\n",
        "  flow_p10PktSize = np.concatenate(flow_p10PktSize)\n",
        "  flow_p50PktSize = np.concatenate(flow_p50PktSize)\n",
        "  flow_p90PktSize = np.concatenate(flow_p90PktSize)\n",
        "  flow_pkts_per_burst = np.concatenate(flow_pkts_per_burst)\n",
        "  flow_ipg_mean = np.concatenate(flow_ipg_mean)\n",
        "  flow_ipg_var = np.concatenate(flow_ipg_var)\n",
        "  ibgs = np.concatenate(ibgs)\n",
        "  flow_length = np.concatenate(flow_length)\n",
        "  link_capacity = np.concatenate(link_capacity)\n",
        "  rate= np.concatenate(rate)\n",
        "  flow_packet_size = np.concatenate(flow_packet_size)\n",
        "  flow_packets = np.concatenate(flow_packets)\n",
        "  flow_type = np.concatenate(flow_type)\n",
        "  delays = np.concatenate(delays)\n",
        "  flow_bitrate_per_burst = np.concatenate(flow_bitrate_per_burst)\n",
        "  max_link_load = np.concatenate(max_link_load)\n",
        "  loads = []\n",
        "  normal_loads = []\n",
        " \n",
        " \n",
        "\n",
        "  step_wise = len(path_to_link)/ max_link_load.shape[0]\n",
        "  stepping = 0\n",
        "\n",
        "  for m in range(len(path_to_link)):\n",
        "      # print(\"m\", torch.tensor(path_to_link[m][:, 0 ]).max(), len(flow_traffic))\n",
        "      loads.append(torch.sum(torch.index_select((torch.tensor(flow_traffic)), 0, torch.tensor(path_to_link[m][:, 0 ]).type(torch.int64))) / (link_capacity[m] * 1e9))\n",
        "      if m>0 and m % step_wise == 0:\n",
        "        #  print(\"changing now\", m  )\n",
        "         stepping += 1\n",
        "      normal_loads.append(loads[-1]/max_link_load[stepping])\n",
        "\n",
        "  loads = torch.tensor(loads)\n",
        "  normal_load = torch.tensor(normal_loads)\n",
        " \n",
        "\n",
        "  # print(loads.shape,normal_load.shape )\n",
        "  loads_capacities = torch.cat(((loads).unsqueeze(1), normal_load.unsqueeze(1), torch.tensor(link_capacity)), dim=1)\n",
        "\n",
        "\n",
        "  width = flow_length.max()\n",
        "  b = []\n",
        "\n",
        "  for i in range(len(link_to_path)):\n",
        "      if len(link_to_path[i]) != width:\n",
        "          x = np.pad(link_to_path[i]+1, (0, width-len(link_to_path[i])), 'constant', constant_values=0)\n",
        "      else:\n",
        "          x = link_to_path[i]+1\n",
        "      b.append(x)\n",
        "  b = np.array(b)\n",
        "  tensor_list = [torch.from_numpy(seq) for seq in path_to_link]\n",
        "  tensor_lens = [len(seq) for seq in path_to_link]\n",
        "\n",
        "  new_data_val.append(((flow_traffic, flow_p10PktSize,flow_p90PktSize, flow_pkts_per_burst, flow_ipg_mean, flow_ipg_var, ibgs, flow_length, link_capacity,rate, flow_packet_size,flow_packets,flow_type, link_to_path, path_to_link, max_link_load, flow_bitrate_per_burst, loads, tensor_lens, tensor_list, b, loads_capacities), delays))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "vwhr0VOooRx0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# random.shuffle(train_list)\n",
        "train_dataset = CustomDataset(new_data)\n",
        "val_dataset = CustomDataset(new_data_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oai6VDq09VLQ",
        "outputId": "983c1ddd-4cb8-4feb-992e-d8b800069359"
      },
      "outputs": [],
      "source": [
        "normalizers = get_mean_std_dict(ds_train, mean_std_scores_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "XBiovvioJGNa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MeanAbsolutePercentageErrorLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanAbsolutePercentageErrorLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Compute the mean absolute percentage error between y_true and y_pred.\n",
        "\n",
        "        Parameters:\n",
        "            y_true (torch.Tensor): Ground truth values.\n",
        "            y_pred (torch.Tensor): Predicted values.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Mean absolute percentage error.\n",
        "        \"\"\"\n",
        "        epsilon = 1e-7\n",
        "        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JkmZn93XNzj"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "tqt-p75a4lVP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "class PathEmbedding(nn.Module):\n",
        "    def __init__(self, path_state_dim):\n",
        "        super(PathEmbedding, self).__init__()\n",
        "        self.path_state_dim = path_state_dim\n",
        "        self.flow_embedding = nn.Sequential(\n",
        "            nn.Linear(13, self.path_state_dim),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(self.path_state_dim, self.path_state_dim),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        # self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for layer in self.flow_embedding:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                # Use LeCun (Xavier) uniform initialization\n",
        "                init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain('selu'))\n",
        "                # init.constant_(layer.weight,1.0)\n",
        "\n",
        "                if layer.bias is not None:\n",
        "                    init.constant_(layer.bias, 0.0)\n",
        "    def forward(self, x):\n",
        "        return self.flow_embedding(x)\n",
        "\n",
        "class LinkEmbedding(nn.Module):\n",
        "    def __init__(self, link_state_dim):\n",
        "        super(LinkEmbedding, self).__init__()\n",
        "        self.link_state_dim = link_state_dim\n",
        "        self.link_embedding = nn.Sequential(\n",
        "            nn.Linear(3, self.link_state_dim),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(self.link_state_dim, self.link_state_dim),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for layer in self.link_embedding:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                # Use LeCun (Xavier) uniform initialization\n",
        "                init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain('selu'))\n",
        "                # init.constant_(layer.weight,1.0)\n",
        "\n",
        "                if layer.bias is not None:\n",
        "                    init.constant_(layer.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.link_embedding(x)\n",
        "\n",
        "class PathReadout(nn.Module):\n",
        "    def __init__(self, path_state_dim, link_state_dim):\n",
        "        super(PathReadout, self).__init__()\n",
        "        self.path_state_dim = path_state_dim\n",
        "        self.link_state_dim = link_state_dim\n",
        "        self.readout_path = nn.Sequential(\n",
        "            nn.Linear(self.path_state_dim, self.link_state_dim // 2),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(self.link_state_dim // 2, self.link_state_dim // 4),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(self.link_state_dim // 4, 1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        # self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for layer in self.readout_path:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                # Use LeCun (Xavier) uniform initialization\n",
        "                init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain('selu'))\n",
        "                # init.constant_(layer.weight,1.0)\n",
        "\n",
        "                if layer.bias is not None:\n",
        "                    init.constant_(layer.bias, 0.0)\n",
        "    def forward(self, x):\n",
        "        return self.readout_path(x)\n",
        "\n",
        "\n",
        "class AttnModel(nn.Module):\n",
        "    def __init__(self, path_state_dim):\n",
        "        super(AttnModel, self).__init__()\n",
        "        self.attn = nn.Sequential(\n",
        "            nn.Linear(path_state_dim, path_state_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.01)\n",
        "        )\n",
        "        # Initialize the weights with zeros\n",
        "        # self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for layer in self.attn:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.ones_(layer.weight)\n",
        "                #  init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain('selu'))\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.ones_(layer.bias)\n",
        "\n",
        "\n",
        "class CustomGRUModel(nn.Module):\n",
        "    def __init__(self, path_state_dim):\n",
        "        super(CustomGRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(path_state_dim, path_state_dim, batch_first=True)\n",
        "        # Initialize the weights with zeros\n",
        "        # self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.ones_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.ones_(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DCKPbKLyPQmP",
        "outputId": "2e5bdc1f-2da3-4c3c-83cf-8407e784787f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1689 [00:00<?, ?it/s]C:\\Users\\gudab\\AppData\\Local\\Temp\\ipykernel_15960\\1752178708.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  loads_capacities = torch.tensor(loads_capacities).to(device)\n",
            "Loss: 98.1488  MSE 7.9789 lr: 0.001: 100%|██████████| 1689/1689 [1:11:22<00:00,  2.54s/it]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss =  155.62538146972656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 40.4625 lr: 0.001: 100%|██████████| 422/422 [01:17<00:00,  5.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val loss =  58.82002258300781\n",
            "4360.18109536171 seconds\n",
            "epoch  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 80.1138  MSE 6.1613 lr: 0.001:  68%|██████▊   | 1151/1689 [17:13<07:23,  1.21it/s] "
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MyCombinedModel(nn.Module):\n",
        "    def __init__(self, link_state_dim, path_state_dim):\n",
        "        super(MyCombinedModel, self).__init__()\n",
        "        self.link_embed = LinkEmbedding(link_state_dim)  # Assuming LinkEmbedding is defined\n",
        "        self.flow_embedded = PathEmbedding(path_state_dim)  # Assuming PathEmbedding is defined\n",
        "        self.reader = PathReadout(path_state_dim, link_state_dim)  # Assuming PathReadout is defined\n",
        "        self.link_update = nn.GRUCell(link_state_dim, link_state_dim)\n",
        "        self.attention = AttnModel(path_state_dim).attn\n",
        "        self.path_update = CustomGRUModel(path_state_dim).gru\n",
        "\n",
        "    def forward(self, flow_length, flow_traffic_unnormalized, link_to_path, path_to_link, link_capacity, all_flows_used, flow_traffic, link_capacity_orig, max_link_load,  loads, tensor_lens, tensor_list, b, loads_capacities):\n",
        "        all_flows_used = all_flows_used.to(device)\n",
        "        path_state = self.flow_embedded(all_flows_used)\n",
        "        flow_traffic_unnormalized = torch.tensor( flow_traffic_unnormalized).to(device)\n",
        "        link_capacity = torch.tensor(link_capacity).to(device)\n",
        "        flow_traffic = torch.tensor(flow_traffic).to(device)\n",
        "        link_capacity_orig = torch.tensor(link_capacity_orig).to(device)\n",
        "\n",
        "\n",
        "        loads_capacities = torch.tensor(loads_capacities).to(device)\n",
        "\n",
        "        link_state = self.link_embed(loads_capacities)\n",
        "        path_state = torch.unsqueeze(path_state, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "        for _ in range(12):\n",
        "            link_state = torch.cat((torch.zeros(1, 16).to(device), link_state), dim=0)\n",
        "            try:\n",
        "                tensor_53x16 = link_state[b]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            path_state_sequence, path_state = self.path_update(tensor_53x16, path_state)\n",
        "            idx = (torch.tensor(flow_length) - 1).view(-1, 1).expand(path_state_sequence.size(0), path_state_sequence.size(2)).unsqueeze(1).to(device)\n",
        "            path_state = path_state_sequence.gather(1, idx.to(torch.int64)).squeeze(dim=1).unsqueeze(dim=0)\n",
        "            prev_path_state = path_state.transpose(0, 1)\n",
        "            path_state_sequence = torch.cat((prev_path_state, path_state_sequence), dim=1)\n",
        "            path_state_sequence_for_indexing = torch.cat((torch.zeros_like(prev_path_state), path_state_sequence), dim=1)\n",
        "\n",
        "            padded_path_to_link = nn.utils.rnn.pad_sequence(tensor_list, batch_first=True).to(device)\n",
        "            path_sum = torch.sum(path_state_sequence_for_indexing[padded_path_to_link[:, :, 0], padded_path_to_link[:, :, 1]], axis=1)\n",
        "            link_state = self.link_update(path_sum, link_state[1:, :])\n",
        "\n",
        "        link_capacity_orig = torch.cat((torch.zeros(1, 1).to(device), (link_capacity_orig)), dim=0)\n",
        "        capacity_gather_53x16 = link_capacity_orig[b]\n",
        "        delays = []\n",
        "        occupancy = self.reader(path_state_sequence[:, 1:])\n",
        "\n",
        "        for i, path_state_seq in enumerate(path_state_sequence):\n",
        "            delays.append(torch.sum(occupancy[i][:flow_length[i]][0] /(capacity_gather_53x16[i][:flow_length[i]]).T[0]).unsqueeze(0))\n",
        "\n",
        "        return torch.cat(delays).to(device)\n",
        "\n",
        "\n",
        "link_state_dim = 16\n",
        "path_state_dim = 16\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "model = MyCombinedModel(link_state_dim,  path_state_dim).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5,patience=5)\n",
        "criterion = MeanAbsolutePercentageErrorLoss()\n",
        "mseloss_c = nn.MSELoss()\n",
        "def train():\n",
        "  model.train()\n",
        "  # fake_batch_size= 1\n",
        "  with tqdm(train_dataset, total=len(train_dataset)) as pbar:\n",
        "    predictions = []\n",
        "    y_all = []\n",
        "    # loss = 0\n",
        "    # loss = torch.tensor([0.0])\n",
        "\n",
        "    for i,batch in enumerate(pbar):\n",
        "\n",
        "        data, labels = batch  #\n",
        "\n",
        "        ##flow_traffic 0, flow_p10PktSize 1,flow_p90PktSize 2, flow_pkts_per_burst 3, flow_ipg_mean 4, flow_ipg_var 5, ibgs 6,\n",
        "        ##flow_length 7, link_capacity 8, rate 9, flow_packet_size 10,\n",
        "        ##flow_packets 11 ,flow_type 12,link_to_path 13, path_to_link 14,\n",
        "        ##max_link_load 15, flow_bitrate_per_burst 16,  loads  17, tensor_lens 18, tensor_list 19, b 20, load_capacities 21\n",
        "        flow_traffic_unnormalized = data[0]\n",
        "        flow_traffic = (data[0] - normalizers[\"flow_traffic\"][0]) *normalizers[\"flow_traffic\"][1]\n",
        "        flow_packets = (data[11]- normalizers[\"flow_packets\"][0]) *normalizers[\"flow_packets\"][1]\n",
        "        max_link_load = data[15]\n",
        "        flow_pkt_per_burst = (data[3] - normalizers[\"flow_pkts_per_burst\"][0]) *normalizers[\"flow_pkts_per_burst\"][1]\n",
        "        flow_bitrate = (data[16] - normalizers[\"flow_bitrate_per_burst\"][0]) *normalizers[\"flow_bitrate_per_burst\"][1]\n",
        "        flow_packet_size = (data[10]- normalizers[\"flow_packet_size\"][0])  * normalizers[\"flow_packet_size\"][1]\n",
        "        flow_type = data[12]\n",
        "        flow_ipg_mean = (data[4]- normalizers[\"flow_ipg_mean\"][0])  * normalizers[\"flow_ipg_mean\"][1]\n",
        "        flow_ipg_var = (data[5] - normalizers[\"flow_ipg_var\"][0])  * normalizers[\"flow_ipg_var\"][1]\n",
        "\n",
        "        cbr_rate = (data[9] - normalizers[\"rate\"][0]) *normalizers[\"rate\"][1]\n",
        "\n",
        "        flow_length = data[7]\n",
        "        ibg = (data[6] - normalizers[\"ibg\"][0]) *normalizers[\"ibg\"][1]\n",
        "        flow_p90pktsize = (data[2] - normalizers[\"flow_p90PktSize\"][0]) *normalizers[\"flow_p90PktSize\"][1]\n",
        "\n",
        "        link_capacity = (data[8] - normalizers[\"link_capacity\"][0]) *normalizers[\"link_capacity\"][1]\n",
        "        link_capacity_orig = data[8]\n",
        "        link_to_path = data[13]\n",
        "      \n",
        "        path_to_link = data[14]\n",
        "        flow_pkt_size_normal = flow_packets\n",
        "\n",
        "        loads = data[17]\n",
        "        tensor_lens = data[18]\n",
        "        tensor_list = data[19]\n",
        "        b = data[20]\n",
        "        loads_capacities = data[21]\n",
        "        # print(loads_capacities.shape)\n",
        "        y_vals = torch.tensor(labels).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        all_flows_used = torch.tensor(np.concatenate([flow_traffic, ibg, cbr_rate, flow_p90pktsize, flow_packets, flow_type,  flow_packet_size, flow_bitrate, flow_ipg_mean, flow_ipg_var, flow_pkt_per_burst,    np.expand_dims(flow_length,axis=1)], axis=1,dtype=np.float32 ) )\n",
        "\n",
        "        # all_flows_used = torch.tensor(np.concatenate([flow_traffic,flow_packets, flow_type, flow_type, np.expand_dims(flow_length,axis=1)], axis=1,dtype=np.float32 ) )\n",
        "\n",
        "\n",
        "\n",
        "        outputs = model(flow_length, flow_traffic_unnormalized, link_to_path, path_to_link, link_capacity,all_flows_used, flow_traffic, link_capacity_orig,max_link_load, loads, tensor_lens, tensor_list, b, loads_capacities)\n",
        "\n",
        "        predictions.extend(outputs.cpu().detach().numpy())\n",
        "        # print(\"predictions\",predictions[:5])\n",
        "\n",
        "        loss = criterion(outputs, y_vals ) # Compute the loss.\n",
        "\n",
        "\n",
        "        # if i%fake_batch_size == 0 or i== len(train_dataset):\n",
        "        mseloss = torch.log(mseloss_c(outputs, y_vals))\n",
        "        loss.backward()  # Derive gradients.\n",
        "        # mseloss.backward()\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "        # loss = torch.tensor([0.0])\n",
        "        # print(\"outputs\", outputs )\n",
        "        pbar.set_description(f\"Loss: {loss:.4f}  MSE {mseloss:.4f} lr: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
        "        y_all.extend(labels)\n",
        "    train_loss = criterion(torch.tensor(predictions), torch.tensor(y_all )).item()\n",
        "    print(\"train loss = \",train_loss)\n",
        "    # Print the gradients of the parameters before and after optimization step\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     print(f'Parameter: {name}, Gradient Norm: {param.norm()}')\n",
        "\n",
        "    model.eval()\n",
        "    with tqdm(val_dataset, total=len(val_dataset)) as pbar:\n",
        "      predictions = []\n",
        "      y_all = []\n",
        "      for batch in  pbar:\n",
        "              # print(batch)\n",
        "              # Process the batch as needed\n",
        "          data, labels = batch  #\n",
        "\n",
        "          ##flow_traffic 0, flow_p10PktSize 1,flow_p90PktSize 2, flow_pkts_per_burst 3, flow_ipg_mean 4, flow_ipg_var 5, ibgs 6,\n",
        "          ##flow_length 7, link_capacity 8, rate 9, flow_packet_size 10,\n",
        "          ##flow_packets 11 ,flow_type 12,link_to_path 13, path_to_link 14,\n",
        "          ##max_link_load 15, flow_bitrate_per_burst 16,  loads  17, tensor_lens 18, tensor_list 19, b 20, load_capacities 21\n",
        "        \n",
        "          flow_traffic_unnormalized = data[0]\n",
        "\n",
        "          flow_traffic = (data[0] - normalizers[\"flow_traffic\"][0]) *normalizers[\"flow_traffic\"][1]\n",
        "          flow_packets = (data[11]- normalizers[\"flow_packets\"][0]) *normalizers[\"flow_packets\"][1]\n",
        "          max_link_load = data[15]\n",
        "          flow_pkt_per_burst = (data[3] - normalizers[\"flow_pkts_per_burst\"][0]) *normalizers[\"flow_pkts_per_burst\"][1]\n",
        "          flow_bitrate = (data[16] - normalizers[\"flow_bitrate_per_burst\"][0]) *normalizers[\"flow_bitrate_per_burst\"][1]\n",
        "          flow_packet_size = (data[10]- normalizers[\"flow_packet_size\"][0])  * normalizers[\"flow_packet_size\"][1]\n",
        "          flow_type = data[12]\n",
        "          flow_ipg_mean = (data[4]- normalizers[\"flow_ipg_mean\"][0])  * normalizers[\"flow_ipg_mean\"][1]\n",
        "          flow_ipg_var = (data[5] - normalizers[\"flow_ipg_var\"][0])  * normalizers[\"flow_ipg_var\"][1]\n",
        "\n",
        "          cbr_rate = (data[9] - normalizers[\"rate\"][0]) *normalizers[\"rate\"][1]\n",
        "\n",
        "          flow_length = data[7]\n",
        "          ibg = (data[6] - normalizers[\"ibg\"][0]) *normalizers[\"ibg\"][1]\n",
        "          flow_p90pktsize = (data[2] - normalizers[\"flow_p90PktSize\"][0]) *normalizers[\"flow_p90PktSize\"][1]\n",
        "\n",
        "          link_capacity = (data[8] - normalizers[\"link_capacity\"][0]) *normalizers[\"link_capacity\"][1]\n",
        "          link_capacity_orig = data[8]\n",
        "          link_to_path = data[13]\n",
        "          path_to_link = data[14]\n",
        "          flow_pkt_size_normal = flow_packets\n",
        "\n",
        "          loads = data[17]\n",
        "          tensor_lens = data[18]\n",
        "          tensor_list = data[19]\n",
        "          b = data[20]\n",
        "          loads_capacities = data[21]\n",
        "\n",
        "          y_vals = torch.tensor(labels).to(device)\n",
        "\n",
        "\n",
        "\n",
        "          all_flows_used = torch.tensor(np.concatenate([flow_traffic, ibg, cbr_rate, flow_p90pktsize, flow_packets, flow_type,  flow_packet_size, flow_bitrate, flow_ipg_mean, flow_ipg_var, flow_pkt_per_burst,    np.expand_dims(flow_length,axis=1)], axis=1,dtype=np.float32 ) )\n",
        "\n",
        "          # all_flows_used = torch.tensor(np.concatenate([flow_traffic,flow_packets, flow_type, flow_type, np.expand_dims(flow_length,axis=1)], axis=1,dtype=np.float32 ) )\n",
        "\n",
        "\n",
        "\n",
        "          outputs = model(flow_length, flow_traffic_unnormalized, link_to_path, path_to_link, link_capacity,all_flows_used, flow_traffic, link_capacity_orig,max_link_load, loads, tensor_lens, tensor_list, b, loads_capacities)\n",
        "\n",
        "          val_loss_interm = criterion(outputs, y_vals)\n",
        "          predictions.extend(outputs.cpu().detach().numpy())\n",
        "          y_all.extend(labels)\n",
        "          pbar.set_description(f\"Loss: {val_loss_interm:.4f} lr: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
        "    val_loss = criterion(torch.tensor(predictions), torch.tensor(y_all ))\n",
        "    scheduler.step(val_loss.item())\n",
        "\n",
        "    print(\"val loss = \",val_loss.item())\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        }, \"checkpoint.pth\")\n",
        "    return train_loss, val_loss.item()\n",
        "val_losses = []\n",
        "tr_losses = []\n",
        "\n",
        "for epoch in range(1, 150):\n",
        "    t0 = time.time()\n",
        "    print(\"epoch \", epoch)\n",
        "    tr, vl = train()\n",
        "    val_losses.append(vl)\n",
        "    tr_losses.append(tr)\n",
        "\n",
        "    print('{} seconds'.format(time.time() - t0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
